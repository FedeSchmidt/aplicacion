[
	{
		"mnist": "El dataset MNIST es un conjunto de imágenes de dígitos escritos a mano que se usa comunmente en el entrenamiento de diversos sistemas que procesen imágenes, como así también en el entrenamiento y testing de sistemas de machine learning.\nEl conjunto contiene 65.000 imágenes, de las cuales en general se destinan 55.000 para entrenamiento y las restantes 10.000 como conjunto de test.\nUna base de datos extendida de MNIST, llamada EMNIST, se publicó en 2017, conteniendo en total 280.000 imágenes.",
		"learning_rate": "El learning ratio es un parámetro real, configurable, que influirá en la minimización de los errores cometidos por la red en su entrenamiento, como también en la precisión y velocidad del proceso.",
		"ciclos": "Las cantidad de ciclos indica las veces que la red debe procesar de forma completa el conjunto de entrenamiento.",
		"optimizador": "El optimizador es el algoritmo que permite minimizar progresivamente el error cometido por la RNA durante el entrenamiento. La elección del optimizador puede basarse en resultados obtenidos a partir de experimentos y en el tipo de problema que se intenta resolver.",
		"overfitting": "Overfitting es un fenómeno que ocurre cuando el algoritmo de aprendizaje se ajusta de manera excesiva a los datos de entrenamiento y se vuelve incapaz de alcanzar la misma performance al procesar datos nuevos.",
		"metric": "La métrica es una función que permite verificar el error general cometido al entrenar y evaluar la evolución de la red.",
		"tipos": "En la aplicación contemplamos dos tipos de redes neuronales: redes simples y convolucionales.\nLas primeras son las más sencillas que podemos construir y permiten resolver un gran espectro de problemas, pero no son las mejores para trabajar en reconocimiento de patrones en imágenes.\nLas redes neuronales convolucionales se caracterizan por tener al menos una capa convolucional - acompañada con una capa pooling - y son especialmente útiles al trabajar con imágenes ya que pueden aprender características presentes en las mismas.\nExisten distintos tipos de capas que se pueden introducir en una RNA, dependiendo de su tipo. Aquí permitimos utilizar tres de ellas para una red neuronal simple: Flatten, Dense y Dropout. Para una red neuronal convolucional agregamos además capas Convolucionales y de Pooling.\nFlatten posibilita representar una matriz de información como un arreglo.\nUna capa Dense es aquella donde todas las neuronas de la capa se conectan con todas las neuronas de la capa siguiente.\nUna capa Dropout permite eliminar conexiones entre neuronas de dos capas consecutivas.",
		"cost_function": "La función de costo da el error cometido por la red al procesar el conjunto de entrenamiento.\nA través de la configuración de los demás parámetros, se intenta hallar un mínimo para esta función, es decir lograr el menor error posible.",
		"batch_size": "Este parámetro permite dividir el conjunto de entrenamiento en partes. Por defecto, para el problema de MNIST, si no se ha configurado se toma un batch de 32 elementos.\nSe puede elegir cualquier valor pero se prefieren valores no muy grandes.",
		"estructura": "Una RNA es un grafo dirigido, donde los nodos se denominan neuronas artificiales y están conectados por arcos.\nCada nodo recibe valores de los arcos entrantes, computa un nuevo valor según la función de activación de esa neurona y lo emite por sus arcos salientes.\nLas neuronas se agrupan en capas que se pueden distinguir, según su ubicación, en tres categorías: la capa de entrada, las capas ocultas y la capa de salida.",
		"activacion": "La función de activación permitirá a una neurona artificial convertir las entradas que recibe en sus salidas. Cada función tiene beneficios y desventajas en cuanto a la precisión y la velocidad del aprendizaje.\nLa neurona más básica se denomina Perceptrón, que emite como salida solo 0 y 1, y neuronas más útiles son las Sigmoides, ReLU, Lineales y Softmax.\nRespecto a la función Softmax, una neurona de este tipo emite resultados realizando una distribución de probabilidades, con lo cual solo se utiliza en la capa de salida de la red.",
		"construccion": "Para la construcción de una RNA, la capa de entrada se configura de acuerdo al formato de los datos que se tengan en el dataset.\nLa capa de salida también depende del problema y su configuración depende de la forma en que se desee que la red emita sus resultados.\nLas capas ocultas se configuran de manera más experimental, realizando modificaciones y buscando los mejores resultados posibles. Se pueden utilizar problemas similares ya resueltos como guías en la contrucción de una RNA para un nuevo problema."
	}
]
